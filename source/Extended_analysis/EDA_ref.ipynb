{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performing Exploratory Data analysis (EDA) on the data file.\n",
    "The steps would involve:\n",
    "\n",
    "    1. Import Libraries and Load the Data\n",
    "    2. Inspect the Data\n",
    "    3. Handle Missing Values\n",
    "    4. Check for Duplicate Rows\n",
    "    5. Data Visualization\n",
    "    6. Outlier Detection\n",
    "    7. Feature Engineering\n",
    "    8. Time Series Data\n",
    "    9. Save the Cleaned Data\n",
    "    10. Summarize the findings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "About Data:\n",
    "Data is acquired from survey, Ideally it has 222 columns more comments on the columns are documented on the Excel sheet.\n",
    "the idea is to understand the possible deciding factors that makes IBT to decide to quit their current IBT position to nonIBT position.\n",
    "the current paper has proven its hyothesis using COR and CH framework using :structural equation modeling (SEM) and latent moder-\n",
    "ated structural equation modeling (LMS)\n",
    "\n",
    "Understanding on how to convert this set up to ML suitable model and prove/disprove hypothesis is the task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# file_data=pd.read_csv('wavedata.csv', encoding='utf-8') #fails due to mismatched encoding\n",
    "file_data = pd.read_csv('../wavedata.csv', encoding='latin1') # the change of default encoding removes the error caused earlier, that is due to the presence of unique characters to the text formatting in Latin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Latin-1 (ISO-8859-1) encoding for data.\n",
    "Use Latin-1 for comments if:\n",
    "\n",
    "The dataset is limited to Western European characters.\n",
    "As the working data could be with legacy systems or need compatibility with older databases.\n",
    "Performance or memory efficiency is critical.\n",
    "For modern and scalable solutions, however, UTF-8 is generally recommended.\n",
    "\n",
    "But as the document follows the standard of Latin-1 or 'encoding': 'ISO-8859-1' hence this will be default for the project, henceforth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.3.1 -> 24.3.1\n",
      "[notice] To update, run: C:\\Users\\reema\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: chardet in c:\\users\\reema\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (5.2.0)\n",
      "{'encoding': 'ISO-8859-1', 'confidence': 0.73, 'language': ''}\n"
     ]
    }
   ],
   "source": [
    "# Validating the encoding after utf-8 failed.\n",
    "!pip install chardet\n",
    "\n",
    "import chardet\n",
    "\n",
    "# Detect encoding\n",
    "with open('../wavedata.csv', 'rb') as file:\n",
    "    result = chardet.detect(file.read())\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Result from the above code for documentation:\n",
    "{'encoding': 'ISO-8859-1', 'confidence': 0.73, 'language': ''}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STEP 1: Import Libraries and Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   travfreq  aircrew  empl  jobsat1  jobsat2  jobsat3  roleover1  roleover2  \\\n",
      "0         3      2.0     1        4        4        4          3          4   \n",
      "1         3      2.0     1        4        4        5          4          4   \n",
      "2         3      2.0     1        4        3        3          4          4   \n",
      "3         3      2.0     1        4        4        4          2          3   \n",
      "4         4      2.0     1        4        4        4          3          4   \n",
      "\n",
      "   roleover3  workadj1  ...  yearstravel  numbertravel  daysknowtravel  \\\n",
      "0          4         5  ...          1.0             1              10   \n",
      "1          4         5  ...          1.0             3              14   \n",
      "2          4         4  ...          3.0             1              30   \n",
      "3          4         4  ...          5.0             1              15   \n",
      "4          3         4  ...         15.0             1              14   \n",
      "\n",
      "   meettravel  flytravel   countrytravel  daystrip  reasontrip  \\\n",
      "0           2          1          Europe       5.0           4   \n",
      "1           4          2  europe, france       4.0           1   \n",
      "2           5          2          Europe       5.0           6   \n",
      "3           3          2         Ireland       5.0           2   \n",
      "4           5          2           china      14.0           2   \n",
      "\n",
      "          otherreasontrip  weekendtrip  \n",
      "0                     NaN            1  \n",
      "1                     NaN            2  \n",
      "2  Meeting with Collegues            3  \n",
      "3                     NaN            1  \n",
      "4                     NaN            3  \n",
      "\n",
      "[5 rows x 222 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load the CSV file with the detected encoding\n",
    "# This is the raw data used for the statistical analysis.\n",
    "df = pd.read_csv('../wavedata.csv', encoding='ISO-8859-1')\n",
    "\n",
    "# Display the data to verify it loads correctly\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['travfreq', 'aircrew', 'empl', 'jobsat1', 'jobsat2', 'jobsat3', 'roleover1', 'roleover2', 'roleover3', 'workadj1', 'workadj2', 'workadj3', 'workadj4', 'workadj5', 'workadj6', 'workadj7', 'workadj8', 'workadj9', 'turnover1', 'turnover2', 'turnover3', 'wneglect1', 'wneglect2', 'wneglect3', 'wneglect4', 'wneglect5', 'wneglect6', 'wsitcon1', 'wsitcon2', 'wsitcon3', 'wsitcon4', 'wrclar1', 'wrclar2', 'wrclar3', 'wrclar4', 'wrclar5', 'wrclar6', 'wtimepres1', 'wtimepres2', 'wtimepres3', 'wtimepres4', 'wnovel1', 'wnovel2', 'wnovel3', 'wnovel4', 'wcowinsexphelp 1', 'wcowinsexphelp 2', 'wcowinsexphelp 3', 'wcowinsexphelp 4', 'wroleconflict 1', 'wroleconflict 2', 'wroleconflict 3', 'wintconflictd 1', 'wintconflictd 2', 'wintconflictd 3', 'wintconflictd 4', 'wintconflictf 1', 'wintconflictf 2', 'wintconflictf 3', 'wemotdem 1', 'wemotdem 2', 'wemotdem 3', 'wemotdem 4', 'wemotdem 5', 'wpeopresp1', 'wpeopresp2', 'wpeopresp3', 'wpeopresp4', 'wpeopresp5', 'wallcowexp1', 'wallcowexp2', 'wallcowexp3', 'wallcowexp4', 'wallcowexp5', 'wallcowexp6', 'wallcowexp7', 'wallcowexp8', 'wallcowexp9', 'wallcowexp10', 'wallcowexp11', 'wallcowexp12', 'cowint', 'cowdep', 'wteams', 'wcom', 'Unnamed: 85', 'fsat1', 'fsat2', 'fsat3', 'fneglect1', 'fneglect2', 'fneglect3', 'fneglect4', 'fneglect5', 'fneglect6', 'fadj1', 'fadj2', 'fadj3', 'fadj4', 'fadj5', 'fadj6', 'fadj7', 'fadj8', 'fadj9', 'wfc1', 'wfc2', 'wfc3', 'wfc4', 'wfc5', 'ftimepres1', 'ftimepres2', 'ftimepres3', 'ftimepres4', 'check2', 'faminstexp1', 'faminstexp2', 'faminstexp3', 'faminstexp4', 'femotdem1', 'femotdem2', 'femotdem3', 'femotdem4', 'froleover1', 'froleover2', 'froleover3', 'froleconf1', 'froleconf2', 'froleconf3', 'fintconfd1', 'fintconfd2', 'fintconfd3', 'fintconfd4', 'fintconff1', 'fintconff2', 'fintconff3', 'famallexp1', 'famallexp2', 'famallexp3', 'famallexp4', 'famallexp5', 'famallexp6', 'famallexp7', 'famallexp8', 'famallexp9', 'famallexp10', 'famallexp11', 'famallexp12', 'burn1', 'burn2', 'burn3', 'burn4', 'burn5', 'burn6', 'burn7', 'burn8', 'burn9', 'burn10', 'burn11', 'burn12', 'burn13', 'burn14', 'burn15', 'burn16', 'burn17', 'burn18', 'burn19', 'burn20', 'burn21', 'supsup1', 'cowsup1', 'parsup1', 'famsup1', 'supsup2', 'cowsup2', 'parsup2', 'famsup2', 'supsup3', 'cowsup3', 'parsup3', 'famsup3', 'supsup4', 'cowsup4', 'parsup4', 'famsup4', 'cogflex1', 'cogflex2', 'cogflex3', 'cogflex4', 'cogflex5', 'cogflex6', 'cogflex7', 'cogflex8', 'cogflex9', 'cogflex10', 'cogflex11', 'cogflex12', 'houswork', 'childwork', 'marstat', 'children', 'whours', 'tenure', 'position', 'education', 'gender', 'age', 'industry', 'otherindustry', 'nationality', 'residence', 'lang', 'feeltravel', 'yearstravel', 'numbertravel', 'daysknowtravel', 'meettravel', 'flytravel', 'countrytravel', 'daystrip', 'reasontrip', 'otherreasontrip', 'weekendtrip']\n",
      "travfreq           579\n",
      "aircrew            524\n",
      "empl               579\n",
      "jobsat1            579\n",
      "jobsat2            579\n",
      "                  ... \n",
      "countrytravel      578\n",
      "daystrip           577\n",
      "reasontrip         579\n",
      "otherreasontrip     38\n",
      "weekendtrip        579\n",
      "Length: 222, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print((df.columns).to_list())\n",
    "column_count=(df.columns).to_list()\n",
    "print(df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "222\n"
     ]
    }
   ],
   "source": [
    "a=['travfreq', 'aircrew', 'empl', 'jobsat1', 'jobsat2', 'jobsat3', 'roleover1', 'roleover2', 'roleover3', 'workadj1', 'workadj2', 'workadj3', 'workadj4', 'workadj5', 'workadj6', 'workadj7', 'workadj8', 'workadj9', 'turnover1', 'turnover2', 'turnover3', 'wneglect1', 'wneglect2', 'wneglect3', 'wneglect4', 'wneglect5', 'wneglect6', 'wsitcon1', 'wsitcon2', 'wsitcon3', 'wsitcon4', 'wrclar1', 'wrclar2', 'wrclar3', 'wrclar4', 'wrclar5', 'wrclar6', 'wtimepres1', 'wtimepres2', 'wtimepres3', 'wtimepres4', 'wnovel1', 'wnovel2', 'wnovel3', 'wnovel4', 'wcowinsexphelp 1', 'wcowinsexphelp 2', 'wcowinsexphelp 3', 'wcowinsexphelp 4', 'wroleconflict 1', 'wroleconflict 2', 'wroleconflict 3', 'wintconflictd 1', 'wintconflictd 2', 'wintconflictd 3', 'wintconflictd 4', 'wintconflictf 1', 'wintconflictf 2', 'wintconflictf 3', 'wemotdem 1', 'wemotdem 2', 'wemotdem 3', 'wemotdem 4', 'wemotdem 5', 'wpeopresp1', 'wpeopresp2', 'wpeopresp3', 'wpeopresp4', 'wpeopresp5', 'wallcowexp1', 'wallcowexp2', 'wallcowexp3', 'wallcowexp4', 'wallcowexp5', 'wallcowexp6', 'wallcowexp7', 'wallcowexp8', 'wallcowexp9', 'wallcowexp10', 'wallcowexp11', 'wallcowexp12', 'cowint', 'cowdep', 'wteams', 'wcom', 'Unnamed: 85', 'fsat1', 'fsat2', 'fsat3', 'fneglect1', 'fneglect2', 'fneglect3', 'fneglect4', 'fneglect5', 'fneglect6', 'fadj1', 'fadj2', 'fadj3', 'fadj4', 'fadj5', 'fadj6', 'fadj7', 'fadj8', 'fadj9', 'wfc1', 'wfc2', 'wfc3', 'wfc4', 'wfc5', 'ftimepres1', 'ftimepres2', 'ftimepres3', 'ftimepres4', 'check2', 'faminstexp1', 'faminstexp2', 'faminstexp3', 'faminstexp4', 'femotdem1', 'femotdem2', 'femotdem3', 'femotdem4', 'froleover1', 'froleover2', 'froleover3', 'froleconf1', 'froleconf2', 'froleconf3', 'fintconfd1', 'fintconfd2', 'fintconfd3', 'fintconfd4', 'fintconff1', 'fintconff2', 'fintconff3', 'famallexp1', 'famallexp2', 'famallexp3', 'famallexp4', 'famallexp5', 'famallexp6', 'famallexp7', 'famallexp8', 'famallexp9', 'famallexp10', 'famallexp11', 'famallexp12', 'burn1', 'burn2', 'burn3', 'burn4', 'burn5', 'burn6', 'burn7', 'burn8', 'burn9', 'burn10', 'burn11', 'burn12', 'burn13', 'burn14', 'burn15', 'burn16', 'burn17', 'burn18', 'burn19', 'burn20', 'burn21', 'supsup1', 'cowsup1', 'parsup1', 'famsup1', 'supsup2', 'cowsup2', 'parsup2', 'famsup2', 'supsup3', 'cowsup3', 'parsup3', 'famsup3', 'supsup4', 'cowsup4', 'parsup4', 'famsup4', 'cogflex1', 'cogflex2', 'cogflex3', 'cogflex4', 'cogflex5', 'cogflex6', 'cogflex7', 'cogflex8', 'cogflex9', 'cogflex10', 'cogflex11', 'cogflex12', 'houswork', 'childwork', 'marstat', 'children', 'whours', 'tenure', 'position', 'education', 'gender', 'age', 'industry', 'otherindustry', 'nationality', 'residence', 'lang', 'feeltravel', 'yearstravel', 'numbertravel', 'daysknowtravel', 'meettravel', 'flytravel', 'countrytravel', 'daystrip', 'reasontrip', 'otherreasontrip', 'weekendtrip']\n",
    "print(len(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STEP 2: Inspect the Data\n",
    "\n",
    "Questions to answer:\n",
    "    1. Shape:\n",
    "        Number of rows and columns: Shape of dataset: (579, 222)\n",
    "    2. Data Types:\n",
    "        Identify numeric, categorical, and datetime columns.\n",
    "    3. Missing Values:\n",
    "        Count and percentage of missing values in each column.\n",
    "    4. Summary Statistics:\n",
    "        Get an overview of the data distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of dataset: (579, 222)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 579 entries, 0 to 578\n",
      "Columns: 222 entries, travfreq to weekendtrip\n",
      "dtypes: float64(18), int64(187), object(17)\n",
      "memory usage: 1004.3+ KB\n",
      "None\n",
      "         travfreq  aircrew   empl     jobsat1     jobsat2     jobsat3  \\\n",
      "count  579.000000    524.0  579.0  579.000000  579.000000  579.000000   \n",
      "mean     3.447323      2.0    1.0    4.243523    4.234888    4.217617   \n",
      "std      0.881513      0.0    0.0    0.834655    0.827778    0.772114   \n",
      "min      2.000000      2.0    1.0    1.000000    1.000000    1.000000   \n",
      "25%      3.000000      2.0    1.0    4.000000    4.000000    4.000000   \n",
      "50%      3.000000      2.0    1.0    4.000000    4.000000    4.000000   \n",
      "75%      4.000000      2.0    1.0    5.000000    5.000000    5.000000   \n",
      "max      5.000000      2.0    1.0    5.000000    5.000000    5.000000   \n",
      "\n",
      "        roleover1   roleover2   roleover3    workadj1  ...   education  \\\n",
      "count  579.000000  579.000000  579.000000  579.000000  ...  576.000000   \n",
      "mean     3.347150    3.160622    3.236615    4.314335  ...    3.229167   \n",
      "std      1.216398    1.256170    1.223037    0.785675  ...    1.029774   \n",
      "min      1.000000    1.000000    1.000000    1.000000  ...    1.000000   \n",
      "25%      2.000000    2.000000    2.000000    4.000000  ...    3.000000   \n",
      "50%      3.000000    3.000000    3.000000    4.000000  ...    3.000000   \n",
      "75%      4.000000    4.000000    4.000000    5.000000  ...    4.000000   \n",
      "max      5.000000    5.000000    5.000000    5.000000  ...    5.000000   \n",
      "\n",
      "           gender         age    industry  feeltravel  yearstravel  \\\n",
      "count  579.000000  579.000000  578.000000  579.000000   579.000000   \n",
      "mean     1.392055   37.970639    8.397924    1.461140     6.800518   \n",
      "std      0.488631   10.374852    3.332129    0.720289     6.201101   \n",
      "min      1.000000   18.000000    1.000000    1.000000     0.000000   \n",
      "25%      1.000000   30.000000    5.000000    1.000000     2.000000   \n",
      "50%      1.000000   35.000000    9.000000    1.000000     5.000000   \n",
      "75%      2.000000   44.000000   12.000000    2.000000    10.000000   \n",
      "max      2.000000   73.000000   13.000000    3.000000    40.000000   \n",
      "\n",
      "        flytravel    daystrip  reasontrip  weekendtrip  \n",
      "count  579.000000  577.000000  579.000000   579.000000  \n",
      "mean     1.880829    6.289428    2.804836     1.595855  \n",
      "std      0.656057    5.665543    1.699250     0.851855  \n",
      "min      1.000000    0.000000    1.000000     1.000000  \n",
      "25%      1.000000    4.000000    1.000000     1.000000  \n",
      "50%      2.000000    5.000000    2.000000     1.000000  \n",
      "75%      2.000000    7.000000    4.000000     2.000000  \n",
      "max      3.000000   90.000000    6.000000     3.000000  \n",
      "\n",
      "[8 rows x 205 columns]\n",
      "Missing Values:\n",
      " travfreq             0\n",
      "aircrew             55\n",
      "empl                 0\n",
      "jobsat1              0\n",
      "jobsat2              0\n",
      "                  ... \n",
      "countrytravel        1\n",
      "daystrip             2\n",
      "reasontrip           0\n",
      "otherreasontrip    541\n",
      "weekendtrip          0\n",
      "Length: 222, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Shape and info\n",
    "print(f\"Shape of dataset: {df.shape}\")\n",
    "print(df.info())\n",
    "\n",
    "# Summary statistics\n",
    "print(df.describe())\n",
    "\n",
    "# Check for missing values\n",
    "missing_data = df.isnull().sum()\n",
    "print(\"Missing Values:\\n\", missing_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_columns = None\n",
    "pd.options.display.max_rows = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The actual count of missing values in the dataset, to get an estimate of which column contains the most missing values\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "aircrew             55\n",
       "turnover1            1\n",
       "turnover2            2\n",
       "turnover3            3\n",
       "wneglect1            2\n",
       "wneglect2            4\n",
       "wneglect3            1\n",
       "wneglect4            2\n",
       "wneglect5            2\n",
       "wneglect6            3\n",
       "wpeopresp3           3\n",
       "wpeopresp4           2\n",
       "wpeopresp5           3\n",
       "wteams               1\n",
       "wcom                 2\n",
       "houswork             2\n",
       "childwork            7\n",
       "whours               2\n",
       "tenure              15\n",
       "education            3\n",
       "industry             1\n",
       "otherindustry      468\n",
       "nationality          5\n",
       "residence            5\n",
       "lang               108\n",
       "countrytravel        1\n",
       "daystrip             2\n",
       "otherreasontrip    541\n",
       "dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('The actual count of missing values in the dataset, to get an estimate of which column contains the most missing values')\n",
    "\n",
    "df[df.columns[df.isnull().any()]].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['otherindustry','lang','otherreasontrip','aircrew'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After deleting columns after discussion\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "turnover1         1\n",
       "turnover2         2\n",
       "turnover3         3\n",
       "wneglect1         2\n",
       "wneglect2         4\n",
       "wneglect3         1\n",
       "wneglect4         2\n",
       "wneglect5         2\n",
       "wneglect6         3\n",
       "wpeopresp3        3\n",
       "wpeopresp4        2\n",
       "wpeopresp5        3\n",
       "wteams            1\n",
       "wcom              2\n",
       "houswork          2\n",
       "childwork         7\n",
       "whours            2\n",
       "tenure           15\n",
       "education         3\n",
       "industry          1\n",
       "nationality       5\n",
       "residence         5\n",
       "countrytravel     1\n",
       "daystrip          2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('After deleting columns after discussion')\n",
    "\n",
    "df[df.columns[df.isnull().any()]].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float64')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['tenure'].dtype\n",
    "df['tenure'] = pd.to_numeric(df['tenure'])\n",
    "df['tenure'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "if df['tenure'].dtype in [float, int]:  # Ensure the column is numeric\n",
    "    df['tenure'] = df['tenure'].fillna(df['tenure'].mean())\n",
    "else:\n",
    "    print(\"'tenure' is not numeric and cannot be filled with mean.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After imputing missing values of tenure with avg value \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "turnover1        1\n",
       "turnover2        2\n",
       "turnover3        3\n",
       "wneglect1        2\n",
       "wneglect2        4\n",
       "wneglect3        1\n",
       "wneglect4        2\n",
       "wneglect5        2\n",
       "wneglect6        3\n",
       "wpeopresp3       3\n",
       "wpeopresp4       2\n",
       "wpeopresp5       3\n",
       "wteams           1\n",
       "wcom             2\n",
       "houswork         2\n",
       "childwork        7\n",
       "whours           2\n",
       "education        3\n",
       "industry         1\n",
       "nationality      5\n",
       "residence        5\n",
       "countrytravel    1\n",
       "daystrip         2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('After imputing missing values of tenure with avg value ')\n",
    "\n",
    "df[df.columns[df.isnull().any()]].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The percentatge of missing values in the dataset, to get an estimate of which column contains the most missing values and make a decision on how to fill them\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "turnover1        0.172712\n",
       "turnover2        0.345423\n",
       "turnover3        0.518135\n",
       "wneglect1        0.345423\n",
       "wneglect2        0.690846\n",
       "wneglect3        0.172712\n",
       "wneglect4        0.345423\n",
       "wneglect5        0.345423\n",
       "wneglect6        0.518135\n",
       "wpeopresp3       0.518135\n",
       "wpeopresp4       0.345423\n",
       "wpeopresp5       0.518135\n",
       "wteams           0.172712\n",
       "wcom             0.345423\n",
       "houswork         0.345423\n",
       "childwork        1.208981\n",
       "whours           0.345423\n",
       "education        0.518135\n",
       "industry         0.172712\n",
       "nationality      0.863558\n",
       "residence        0.863558\n",
       "countrytravel    0.172712\n",
       "daystrip         0.345423\n",
       "dtype: float64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('The percentatge of missing values in the dataset, to get an estimate of which column contains the most missing values and make a decision on how to fill them')\n",
    "df[df.columns[df.isnull().any()]].isnull().sum() * 100 / df.shape[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the first result:\n",
    "1. otherreasontrip    : 93%\n",
    "2. otherindustry      : 80%\n",
    "3. lang               : 18%\n",
    "4. aircrew            : 9%\n",
    "5. tenure             : 2%\n",
    "6. childwork          : 1%\n",
    "\n",
    "Are above zero, hence the priority of dealing the columns missing values is documented.\n",
    "\n",
    "This is now handled and the statistics are different as above.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. otherreasontrip    : 93%   need details on how to fill the null values\n",
    "2. otherindustry      : 80%   need to understand if this is important field and if that can be filled with a meaning full value\n",
    "3. lang               : 18%   need to undertand meaningful value to fill it with\n",
    "4. aircrew            : 9%    I believe this isnt priority, need confirmation\n",
    "5. tenure             : 2%    default value? or just drop?\n",
    "6. childwork          : 1%     need info on the column \n",
    "\n",
    "\n",
    "\n",
    "Question to be answered before the next steps:\n",
    "\n",
    "yearstravel:\thow long one has travelled IBT\n",
    "numbertravel:\tnot sure\n",
    "daysknowtravel:\tinfo about travel provided before the scheduled travel\n",
    "meettravel: number of meetings on IBT\n",
    "daystrip:\thow long each trip avg\n",
    "reasontrip:\treasons for the ignore\n",
    "otherreasontrip:\tignore\n",
    "weekendtrip: weekend trip o not\n",
    "\t\t\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Things to do:\n",
    "\n",
    "write a proposal\n",
    "    what do i need for this:\n",
    "    -understanding how to translate the current model to ML\n",
    "    -variables to be used\n",
    "    -interpret latent variables\n",
    "    -evaluation, y variable, x variable, ML task?\n",
    "\n",
    "Missing value filling:\n",
    "-how and which\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data preprocessing:\n",
    "\n",
    "creating a new column, based on row wise avg of the relevant columns.\n",
    "\n",
    "The columns transformed are listed below:\n",
    "1. jobsat1\tjobsat2\tjobsat3--->  average_jobsat\n",
    "2. roleover1\troleover2\troleover3 --->  average_roleover\n",
    "3. workadj1\tworkadj2\tworkadj3\tworkadj4\tworkadj5\tworkadj6\tworkadj7\tworkadj8\tworkadj9 --->average_workadj\n",
    "4. turnover1\tturnover2\tturnover3 ---> average_turnover\n",
    "5. wneglect1\twneglect2\twneglect3\twneglect4\twneglect5\twneglect6---> average_wneglect\n",
    "6.wsitcon1\twsitcon2\twsitcon3\twsitcon4 ---> average_wsitcon\n",
    "7.wrclar1\twrclar2\twrclar3\twrclar4\twrclar5\twrclar6 ---> average_wrclar\n",
    "8.wtimepres1\twtimepres2\twtimepres3\twtimepres4 --->average_wtimepres\n",
    "9. wnovel1\twnovel2\twnovel3\twnovel4 --->average_wnovel\n",
    "10.wcowinsexphelp 1\twcowinsexphelp 2\twcowinsexphelp 3\twcowinsexphelp 4  --->average_wcowinsexphelp\n",
    "11.wroleconflict 1\twroleconflict 2\twroleconflict 3--->average_wroleconflict\n",
    "12.wintconflictd 1\twintconflictd 2\twintconflictd 3\twintconflictd 4-->average_wintconflictd\n",
    "13. wintconflictf 1\twintconflictf 2\twintconflictf 3 --->average_wintconflictf\n",
    "14. wemotdem 1\twemotdem 2\twemotdem 3\twemotdem 4\twemotdem 5 --->average_wemotdem\n",
    "15. wpeopresp1\twpeopresp2\twpeopresp3\twpeopresp4\twpeopresp5 --->average_wpeopresp\n",
    "16. famallexp11\tfamallexp2\tfamallexp3\tfamallexp4\tfamallexp5\tfamallexp6\tfamallexp7\tfamallexp8\tfamallexp9\tfamallexp10\tfamallexp11\tfamallexp12 --->average_famallexp\n",
    "17. wallcowexp1\twallcowexp2\twallcowexp3\twallcowexp4\twallcowexp5\twallcowexp6\twallcowexp7\twallcowexp8\twallcowexp9\twallcowexp10\twallcowexp11\twallcowexp12 --->average_wallcowexp\n",
    "18. burn1\tburn2\tburn3\tburn4\tburn5\tburn6\tburn7\tburn8\tburn9\tburn10\tburn11\tburn12\tburn13\tburn14\tburn15\tburn16\tburn17\tburn18\tburn19\tburn20\tburn21 --->average_burn\n",
    "19. cogflex1\tcogflex2\tcogflex3\tcogflex4\tcogflex5\tcogflex6\tcogflex7\tcogflex8\tcogflex9\tcogflex10\tcogflex11\tcogflex12 --->average_cogflex\n",
    "20. supsup1\tcowsup1\tparsup1\tfamsup1\tsupsup2\tcowsup2\tparsup2\tfamsup2\tsupsup3\tcowsup3\tparsup3\tfamsup3\tsupsup4\tcowsup4\tparsup4\tfamsup4 --->average_\n",
    "21.fsat1\tfsat2\tfsat3 --->average_fsat\n",
    "22.fneglect1\tfneglect2\tfneglect3\tfneglect4\tfneglect5\tfneglect6 --->average_fneglect\n",
    "23.fadj1\tfadj2\tfadj3\tfadj4\tfadj5\tfadj6\tfadj7\tfadj8\tfadj9 --->average_fadj\n",
    "24. wfc1\twfc2\twfc3\twfc4\twfc5 --->average_wfc\n",
    "25.ftimepres1\tftimepres2\tftimepres3\tftimepres4 --->average_ftimepres\n",
    "26.faminstexp1\tfaminstexp2\tfaminstexp3\tfaminstexp4 --->average_faminstexp\n",
    "27.femotdem1\tfemotdem2\tfemotdem3\tfemotdem4--->average_femotdem\n",
    "28.froleover1\tfroleover2\tfroleover3 --->average_froleover\n",
    "29.froleconf1\tfroleconf2\tfroleconf3 --->average_froleconf\n",
    "30.fintconfd1\tfintconfd2\tfintconfd3\tfintconfd4 --->average_fintconfd\n",
    "31.fintconff1\tfintconff2\tfintconff3 --->average_fintconff\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['average_jobsat'] = df[['jobsat1', 'jobsat2', 'jobsat3']].mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['average_roleover'] = df[['roleover1', 'roleover2', 'roleover3']].mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['average_workadj'] = df[['workadj1',\t'workadj2'\t,'workadj3'\t,'workadj4'\t,'workadj5'\t,'workadj6'\t,'workadj7',\t'workadj8',\t'workadj9']].mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['average_turnover'] = df[['turnover1', 'turnover2', 'turnover3']].mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['average_wneglect'] = df[['wneglect1', 'wneglect2', 'wneglect3','wneglect4','wneglect5','wneglect6']].mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['average_wsitcon'] = df[['wsitcon1', 'wsitcon2', 'wsitcon3','wsitcon4']].mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['average_wrclar'] = df[['wrclar1', 'wrclar2', 'wrclar3','wrclar4','wrclar5','wrclar6']].mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['average_wtimepres'] = df[['wtimepres1', 'wtimepres2', 'wtimepres3','wtimepres4']].mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['average_wnovel'] = df[['wnovel1', 'wnovel2', 'wnovel3','wnovel4']].mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['average_wcowinsexphelp'] = df[['wcowinsexphelp 1', 'wcowinsexphelp 2', 'wcowinsexphelp 3','wcowinsexphelp 4']].mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['average_wroleconflict'] = df[['wroleconflict 1', 'wroleconflict 2', 'wroleconflict 3']].mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['average_wintconflictd'] = df[['wintconflictd 1', 'wintconflictd 2', 'wintconflictd 3','wintconflictd 4']].mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['average_wintconflictf'] = df[['wintconflictf 1', 'wintconflictf 2', 'wintconflictf 3']].mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['average_wemotdem'] = df[['wemotdem 1', 'wemotdem 2', 'wemotdem 3','wemotdem 4','wemotdem 5']].mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['average_wpeopresp'] = df[['wpeopresp1', 'wpeopresp2', 'wpeopresp3','wpeopresp4','wpeopresp5']].mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['average_wallcowexp'] = df[['wallcowexp1'\t,'wallcowexp2',\t'wallcowexp3'\t,'wallcowexp4'\t,'wallcowexp5',\t'wallcowexp6',\t'wallcowexp7',\t'wallcowexp8'\t,'wallcowexp9'\t,'wallcowexp10'\t,\t'wallcowexp11', 'wallcowexp12']].mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['average_famallexp'] = df[['famallexp1'\t,'famallexp2',\t'famallexp3'\t,'famallexp4'\t,'famallexp5',\t'famallexp6',\t'famallexp7',\t'famallexp8'\t,'famallexp9'\t,'famallexp10'\t,\t'famallexp11','famallexp12']].mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['average_burn'] = df[['burn1',\t'burn2','burn3'\t,'burn4'\t,'burn5',\t'burn6',\t'burn7'\t,'burn8'\t,'burn9'\t,'burn10'\t,'burn11',\t'burn12',\t'burn13'\t,'burn14',\t'burn15',\t'burn16'\t,'burn17'\t,'burn18',\t'burn19',\t'burn20',\t'burn21']].mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['average_cogflex'] = df[['cogflex1',\t'cogflex2',\t'cogflex3'\t,'cogflex4'\t,'cogflex5',\t'cogflex6',\t'cogflex7',\t'cogflex8'\t,'cogflex9',\t'cogflex10',\t'cogflex11'\t,'cogflex12']].mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['average_supsup'] = df[['supsup1',\t'supsup2',\t'supsup3'\t,'supsup4']].mean(axis=1)\n",
    "df['average_cowsup'] = df[['cowsup1',\t'cowsup2',\t'cowsup3'\t,'cowsup4']].mean(axis=1)\n",
    "df['average_parsup'] = df[['parsup1',\t'parsup2',\t'parsup3'\t,'parsup4']].mean(axis=1)\n",
    "df['average_famsup'] = df[['famsup1',\t'famsup2',\t'famsup3'\t,'famsup4']].mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['average_fadj'] = df[['fadj1','fadj2','fadj3','fadj4','fadj5','fadj6','fadj7','fadj8','fadj9']].mean(axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['average_fsat'] = df[['fsat1',\t'fsat2',\t'fsat3']].mean(axis=1)\n",
    "df['average_fneglect'] = df[['fneglect1',\t'fneglect2',\t'fneglect3'\t,'fneglect4','fneglect5','fneglect6']].mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['average_wfc'] = df[['wfc1',\t'wfc2',\t'wfc3'\t,'wfc4','wfc5']].mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['average_ftimepres'] = df[['ftimepres1',\t'ftimepres2',\t'ftimepres3','ftimepres4']].mean(axis=1)\n",
    "df['average_faminstexp'] = df[['faminstexp1',\t'faminstexp2',\t'faminstexp3'\t,'faminstexp4']].mean(axis=1)\n",
    "df['average_femotdem'] = df[['femotdem1','femotdem2','femotdem3','femotdem4']].mean(axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['average_froleover'] = df[['froleover1',\t'froleover2',\t'froleover3']].mean(axis=1)\n",
    "df['average_froleconf'] = df[['froleconf1',\t'froleconf2',\t'froleconf3']].mean(axis=1)\n",
    "df['average_fintconfd'] = df[['fintconfd1',\t'fintconfd2',\t'fintconfd3','fintconfd4']].mean(axis=1)\n",
    "df['average_fintconff'] = df[['fintconff1',\t'fintconff2',\t'fintconff3']].mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('output.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "one hot encoding:\n",
    "As these values are categorical value the data is preprocessed with onehot encoding:\n",
    "\n",
    "\n",
    "1.empl\n",
    "2.fulltime/parttime\n",
    "3.gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_df = pd.get_dummies(df, columns=['empl','gender'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_df.to_csv('preprocessed_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the Exploratory data analysis, the following path for the experiments were chosen:\n",
    "\n",
    "1. Task for ml: Regression\n",
    "2. First method for model: Randomforest_regression\n",
    "3. The hypothesis is tested in 3 steps in 3 different notebooks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../image0.png'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[49], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Display an image\u001b[39;00m\n\u001b[0;32m      4\u001b[0m image_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../image0.png\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 5\u001b[0m display(\u001b[43mImage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage_path\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\IPython\\core\\display.py:970\u001b[0m, in \u001b[0;36mImage.__init__\u001b[1;34m(self, data, url, filename, format, embed, width, height, retina, unconfined, metadata, alt)\u001b[0m\n\u001b[0;32m    968\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munconfined \u001b[38;5;241m=\u001b[39m unconfined\n\u001b[0;32m    969\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malt \u001b[38;5;241m=\u001b[39m alt\n\u001b[1;32m--> 970\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mImage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    971\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    973\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwidth \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetadata\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwidth\u001b[39m\u001b[38;5;124m'\u001b[39m, {}):\n\u001b[0;32m    974\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwidth \u001b[38;5;241m=\u001b[39m metadata[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwidth\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\IPython\\core\\display.py:327\u001b[0m, in \u001b[0;36mDisplayObject.__init__\u001b[1;34m(self, data, url, filename, metadata)\u001b[0m\n\u001b[0;32m    324\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetadata \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    325\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetadata \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m--> 327\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    328\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_data()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\IPython\\core\\display.py:1005\u001b[0m, in \u001b[0;36mImage.reload\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1003\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Reload the raw data from file or URL.\"\"\"\u001b[39;00m\n\u001b[0;32m   1004\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed:\n\u001b[1;32m-> 1005\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mImage\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1006\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretina:\n\u001b[0;32m   1007\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retina_shape()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\IPython\\core\\display.py:353\u001b[0m, in \u001b[0;36mDisplayObject.reload\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    351\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilename \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    352\u001b[0m     encoding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_read_flags \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 353\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_flags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m    354\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m    355\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39murl \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    356\u001b[0m     \u001b[38;5;66;03m# Deferred import\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../image0.png'"
     ]
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "# Display an image\n",
    "image_path = \"../image0.png\"\n",
    "display(Image(filename=image_path))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
